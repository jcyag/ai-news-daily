"""
Hugging Face Papers çˆ¬è™«
çˆ¬å– https://huggingface.co/papers è·å–æ¯æ—¥çƒ­é—¨è®ºæ–‡
å¹¶è°ƒç”¨ Arxiv API è·å–è¯¦ç»†æ‘˜è¦
"""
import sys
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import requests
import feedparser
from bs4 import BeautifulSoup

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from crawlers.base import BaseCrawler, NewsItem

logger = logging.getLogger(__name__)


class HuggingFaceCrawler(BaseCrawler):
    """Hugging Face Papers çˆ¬è™«"""
    
    HF_PAPERS_URL = "https://huggingface.co/papers"
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    
    def __init__(self):
        super().__init__("huggingface", "Hugging Face Papers")
    
    def crawl(self) -> List[NewsItem]:
        """çˆ¬å–çƒ­é—¨è®ºæ–‡"""
        # 1. è·å– Hugging Face Papers é¡µé¢
        try:
            response = self._make_request(self.HF_PAPERS_URL)
            paper_ids = self._parse_paper_ids(response.text)
        except Exception as e:
            logger.error(f"[Hugging Face] è·å–é¡µé¢å¤±è´¥: {e}")
            return []
            
        if not paper_ids:
            logger.warning("[Hugging Face] æœªæ‰¾åˆ°è®ºæ–‡ID")
            return []
            
        logger.info(f"[Hugging Face] å‘ç° {len(paper_ids)} ç¯‡çƒ­é—¨è®ºæ–‡ï¼Œæ­£åœ¨è·å–è¯¦æƒ…...")
        
        # 2. è°ƒç”¨ Arxiv API è·å–è¯¦æƒ…
        return self._fetch_arxiv_details(paper_ids)
    
    def _parse_paper_ids(self, html: str) -> List[str]:
        """è§£æé¡µé¢è·å– Arxiv IDåˆ—è¡¨"""
        soup = BeautifulSoup(html, "html.parser")
        ids = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡é“¾æ¥
        # é“¾æ¥æ ¼å¼é€šå¸¸ä¸º /papers/2402.12345
        for link in soup.find_all("a", href=re.compile(r"^/papers/\d+\.\d+")):
            href = link.get("href")
            if href:
                # æå–ID: /papers/2402.12345 -> 2402.12345
                paper_id = href.split("/")[-1]
                if paper_id not in ids:
                    ids.append(paper_id)
        
        # é™åˆ¶æ•°é‡ï¼Œé¿å…è¯·æ±‚è¿‡å¤§
        return ids[:20]
    
    def _fetch_arxiv_details(self, paper_ids: List[str]) -> List[NewsItem]:
        """æ‰¹é‡ä» Arxiv API è·å–è®ºæ–‡è¯¦æƒ…"""
        if not paper_ids:
            return []
            
        # Arxiv API æ”¯æŒ id_list å‚æ•°ï¼Œé€—å·åˆ†éš”
        id_list = ",".join(paper_ids)
        params = {
            "id_list": id_list,
            "max_results": len(paper_ids),
        }
        
        try:
            response = self._make_request(self.ARXIV_API_URL, params=params)
            feed = feedparser.parse(response.content)
            
            items = []
            for entry in feed.entries:
                item = self._parse_arxiv_entry(entry)
                if item:
                    items.append(item)
            
            return items
            
        except Exception as e:
            logger.error(f"[Hugging Face] Arxiv API è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_entry(self, entry) -> Optional[NewsItem]:
        """è§£æ Arxiv æ¡ç›®"""
        title = entry.get("title", "").replace("\n", " ").strip()
        
        # æ‘˜è¦é€šå¸¸åŒ…å«æ¢è¡Œï¼Œæ¸…ç†ä¸€ä¸‹
        summary = entry.get("summary", "").replace("\n", " ").strip()
        
        # å¤„ç†å‘å¸ƒæ—¶é—´
        pub_date = None
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            pub_date = datetime(*entry.published_parsed[:6])
        
        # ä½œè€…åˆ—è¡¨
        authors = [author.get("name") for author in entry.get("authors", [])]
        if authors:
            author_str = ", ".join(authors[:3])
            if len(authors) > 3:
                author_str += " et al."
            summary = f"ğŸ‘¤ {author_str}\n\n{summary}"
        
        # æ„é€  Hugging Face é“¾æ¥ (æ¯” Arxiv é“¾æ¥æ›´å‹å¥½ï¼Œæœ‰è®¨è®ºåŒº)
        # entry.id é€šå¸¸æ˜¯ http://arxiv.org/abs/2402.12345v1
        arxiv_id = entry.get("id", "").split("/abs/")[-1].split("v")[0]
        hf_link = f"https://huggingface.co/papers/{arxiv_id}"
        
        return NewsItem(
            title=title,
            url=hf_link,
            source=self.source_id,
            source_name="Hugging Face Papers",
            pub_date=pub_date,
            summary=summary,
            score=100.0,  # ç»™çƒ­é—¨è®ºæ–‡è¾ƒé«˜çš„é»˜è®¤åˆ†æ•°
        )
